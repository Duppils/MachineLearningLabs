{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-88d96843a926>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-658afd203176>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDictVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from conll_dictorizer import CoNLLDictorizer, Token, save\n",
    "import datasets as ds\n",
    "import sys\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import time\n",
    "from keras import models, layers\n",
    "import sys\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.models import load_model\n",
    "import math\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import LSTM, Bidirectional, SimpleRNN, Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "trace = False\n",
    "path = str(open('path.conf', 'r').read()).strip()\n",
    "\n",
    "OPTIMIZER = 'rmsprop'\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 1\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_SEQUENCE_LENGTH = 150\n",
    "LSTM_UNITS = 512\n",
    "\n",
    "def load(file):\n",
    "    \"\"\"\n",
    "    Return the embeddings in the from of a dictionary\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    file = file\n",
    "    embeddings = {}\n",
    "    glove = open(file)\n",
    "    for line in glove:\n",
    "        values = line.strip().split()\n",
    "        word = values[0]\n",
    "        vector = np.array(values[1:], dtype='float32')\n",
    "        embeddings[word] = vector\n",
    "    glove.close()\n",
    "    embeddings_dict = embeddings\n",
    "    embedded_words = sorted(list(embeddings_dict.keys()))\n",
    "    return embeddings_dict\n",
    "\n",
    "def build_sequences(corpus_dict, key_x='form', key_y='ppos', tolower=True): #key_u='upos' -> 'ppos'\n",
    "    \"\"\"\n",
    "    Creates sequences from a list of dictionaries\n",
    "    :param corpus_dict:\n",
    "    :param key_x:\n",
    "    :param key_y:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    for sentence in corpus_dict:\n",
    "        x = []\n",
    "        y = []\n",
    "        for word in sentence:\n",
    "            x += [word[key_x]]\n",
    "            y += [word[key_y]]\n",
    "        if tolower:\n",
    "            x = list(map(str.lower, x))\n",
    "        X += [x]\n",
    "        Y += [y]\n",
    "    return X, Y\n",
    "\n",
    "def to_index(X, idx):\n",
    "    \"\"\"\n",
    "    Convert the word lists (or POS lists) to indexes\n",
    "    :param X: List of word (or POS) lists\n",
    "    :param idx: word to number dictionary\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X_idx = []\n",
    "    for x in X:\n",
    "        # We map the unknown words to one\n",
    "        x_idx = list(map(lambda x: idx.get(x, 1), x))\n",
    "        X_idx += [x_idx]\n",
    "    return X_idx\n",
    "\n",
    "def predict_sentence(sentence, model, word_idx, \n",
    "                     vocabulary_words, rev_idx_pos, verbose=False):\n",
    "    # Predict one sentence\n",
    "    sentence = sentence.split()\n",
    "    word_idxs = to_index([sentence], word_idx)\n",
    "    word_idx_padded = pad_sequences(word_idxs)\n",
    "\n",
    "    pos_idx_pred = model.predict(word_idx_padded)\n",
    "    # We remove padding\n",
    "    pos_idx_pred = pos_idx_pred[0][-len(sentence):]\n",
    "    pos_idx = list(map(np.argmax, pos_idx_pred))\n",
    "    pos = list(map(rev_idx_pos.get, pos_idx))\n",
    "    if verbose:\n",
    "        print('Sentence', sentence)\n",
    "        print('Sentence word indexes', word_idxs)\n",
    "        print('Padded sentence', word_idx_padded)\n",
    "        print('POS predicted', pos_idx_pred[0])\n",
    "        print('POS shape', pos_idx_pred.shape)\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'regex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e78cc25759ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0m__author__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Pierre Nugues\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mregex\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'regex'"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "\"\"\"\n",
    "CoNLL 2009 file readers and writers for the parts of speech.\n",
    "Version with a class modeled as a vectorizer\n",
    "\"\"\"\n",
    "__author__ = \"Pierre Nugues\"\n",
    "\n",
    "import regex as re\n",
    "\n",
    "\n",
    "def save(file, corpus_dict, column_names):\n",
    "    \"\"\"\n",
    "    Saves the corpus in a file\n",
    "    :param file:\n",
    "    :param corpus_dict:\n",
    "    :param column_names:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with open(file, 'w') as f_out:\n",
    "        for sentence in corpus_dict:\n",
    "            sentence_lst = []\n",
    "            for row in sentence:\n",
    "                items = map(lambda x: row.get(x, '_'), column_names)\n",
    "                sentence_lst += ' '.join(items) + '\\n' #was \\t\n",
    "            sentence_lst += '\\n'\n",
    "            f_out.write(''.join(sentence_lst))\n",
    "\n",
    "\n",
    "class Token(dict):\n",
    "    pass\n",
    "\n",
    "\n",
    "class CoNLLDictorizer:\n",
    "\n",
    "    def __init__(self, column_names, sent_sep='\\n\\n', col_sep=' +'):\n",
    "        self.column_names = column_names\n",
    "        self.sent_sep = sent_sep\n",
    "        self.col_sep = col_sep\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        corpus = corpus.strip()\n",
    "        sentences = re.split(self.sent_sep, corpus)\n",
    "        return list(map(self._split_in_words, sentences))\n",
    "\n",
    "    def fit_transform(self, corpus):\n",
    "        return self.transform(corpus)\n",
    "\n",
    "    def _split_in_words(self, sentence):\n",
    "        rows = re.split('\\n', sentence)\n",
    "        return [Token(dict(zip(self.column_names,\n",
    "                               re.split(self.col_sep, row))))\n",
    "                for row in rows]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_file = 'test.txt'\n",
    "\n",
    "    column_names = ['id', 'form', 'lemma', 'cpos', 'pos', 'feats']\n",
    "    train = open(train_file).read().strip()\n",
    "    conll_dict = CoNLLDictorizer(column_names, col_sep='\\t')\n",
    "    train_dict = conll_dict.transform(train)\n",
    "\n",
    "    print(train_dict[0])\n",
    "    print(train_dict[0][0])\n",
    "    print(type(train_dict[0][0]))\n",
    "    print(train_dict[0][0]['form'])\n",
    "    print(train_dict[1])\n",
    "    tok = Token({'id': '1', 'form': 'La', 'lemma': 'el', 'cpos': 'd', 'pos': 'da', 'feats': 'num=s|gen=f'})\n",
    "    print(tok['form'])\n",
    "    print('form' in tok)\n",
    "\n",
    "    save('out', train_dict, column_names)\n",
    "\n",
    "    tok_dict = {'id': '1', 'form': 'La', 'lemma': 'el', 'cpos': 'd', 'pos': 'da', 'feats': 'num=s|gen=f'}\n",
    "    tok_dict2 = {'id': '1', 'form': 'La', 'lemma': 'el', 'cpos': 'd', 'pos': 'da', 'feats': 'num=s|gen=f'}\n",
    "\n",
    "    tok_set = set(tok_dict)\n",
    "    print(tok_set)\n",
    "\n",
    "    tok_set = tok_set.union(tok_dict2)\n",
    "    print(tok_set)\n",
    "\n",
    "    print(tok.keys())\n",
    "\n",
    "    # exit()\n",
    "    word_set = set()\n",
    "    word_set = set(tok_dict.values())\n",
    "    print(list(word_set))\n",
    "\n",
    "    word_set = set()\n",
    "    word_set = set(tok.values())\n",
    "    print(list(word_set))\n",
    "\n",
    "    word_set = set()\n",
    "    word_set.update(tok.values())\n",
    "    print(list(word_set))\n",
    "\n",
    "    word_set = set()\n",
    "    print(\"Token value:\", tok.values())\n",
    "    word_set = word_set.union(set(tok.values()))\n",
    "    print(list(word_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from conll_dictorizer import CoNLLDictorizer, Token\n",
    "\n",
    "\n",
    "def load_conll2009_pos():\n",
    "    train_file = '../../../corpus/conll2009/en/CoNLL2009-ST-English-train-pos.txt'\n",
    "    dev_file = '../../../corpus/conll2009/en/CoNLL2009-ST-English-development-pos.txt'\n",
    "    test_file = '../../../corpus/conll2009/en/CoNLL2009-ST-test-words-pos.txt'\n",
    "    test2_file = 'simple_pos_test.txt'\n",
    "\n",
    "    column_names = ['id', 'form', 'lemma', 'plemma', 'pos', 'ppos']\n",
    "\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    test2_sentences = open(test2_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names\n",
    "\n",
    "\n",
    "path = str(open(\"path.conf\", \"r\").read()).rstrip('\\n')\n",
    "\n",
    "BASE_DIR = path;\n",
    "\n",
    "\n",
    "def load_conll2003_en():\n",
    "    train_file = BASE_DIR + 'corpus/eng.train'\n",
    "    dev_file = BASE_DIR + 'corpus/eng.valid'\n",
    "    test_file = BASE_DIR + 'corpus/eng.test'\n",
    "    column_names = ['form', 'ppos', 'pchunk', 'ner']\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_sentences, dev_sentences, test_sentences, column_names = load_conll2003_en()\n",
    "\n",
    "    conll_dict = CoNLLDictorizer(column_names, col_sep=' +')\n",
    "    train_dict = conll_dict.transform(train_sentences)\n",
    "    print(train_dict[0])\n",
    "    print(train_dict[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_sentences, dev_sentences, test_sentences, column_names = ds.load_conll2003_en()\n",
    "conll_dict = CoNLLDictorizer(column_names)\n",
    "train_dict = conll_dict.transform(train_sentences)\n",
    "dev_dict = conll_dict.transform(dev_sentences)\n",
    "test_dict = conll_dict.transform(test_sentences)\n",
    "\n",
    "#overwrite column names to include our prediction column\n",
    "column_names.append(\"pred\")\n",
    "\n",
    "#change column names to have our predition column\n",
    "\n",
    "embeddings_file = os.path.join(path, \"corpus/glove.6B.100d.txt\")\n",
    "embeddings_dict = load(embeddings_file)\n",
    "\n",
    "X_train_cat, Y_train_cat = build_sequences(train_dict) # Xfor words, Y for parts of speech (pos)\n",
    "\n",
    "vocabulary_words = sorted(list( set([word for sentence in X_train_cat for word in sentence])))\n",
    "pos = sorted(list(set([pos for sentence in Y_train_cat for pos in sentence])))\n",
    "\n",
    "NB_CLASSES = len(pos)\n",
    "\n",
    "embeddings_words = embeddings_dict.keys()\n",
    "if(trace):\n",
    "    print('Words in GloVe:',  len(embeddings_dict.keys()))\n",
    "vocabulary_words = sorted(list(set(vocabulary_words + list(embeddings_words))))\n",
    "cnt_uniq = len(vocabulary_words) + 2\n",
    "if(trace):\n",
    "    print('# unique words in the vocabulary: embeddings and corpus:', cnt_uniq)\n",
    "\n",
    "# We start at one to make provision for the padding symbol 0 \n",
    "# in RNN and LSTMs and 1 for the unknown words\n",
    "rev_word_idx = dict(enumerate(vocabulary_words, start=2))\n",
    "rev_pos_idx = dict(enumerate(pos, start=2))\n",
    "word_idx = {v: k for k, v in rev_word_idx.items()}\n",
    "pos_idx = {v: k for k, v in rev_pos_idx.items()}\n",
    "print('word index:', list(word_idx.items())[:10])\n",
    "print('POS index:', list(pos_idx.items())[:10])\n",
    "\n",
    "# We create the parallel sequences of indexes\n",
    "X_idx = to_index(X_train_cat, word_idx)\n",
    "Y_idx = to_index(Y_train_cat, pos_idx)\n",
    "print('First sentences, word indices', X_idx[:3])\n",
    "print('First sentences, POS indices', Y_idx[:3])\n",
    "\n",
    "X = pad_sequences(X_idx)\n",
    "Y = pad_sequences(Y_idx)\n",
    "\n",
    "print(X[0])\n",
    "print(Y[0])\n",
    "\n",
    "# The number of POS classes and 0 (padding symbol)\n",
    "Y_train = to_categorical(Y, num_classes=len(pos) + 2)\n",
    "print(Y_train[0])\n",
    "\n",
    "rdstate = np.random.RandomState(1234567)\n",
    "embedding_matrix = rdstate.uniform(-0.05, 0.05, (len(vocabulary_words) + 2, EMBEDDING_DIM))\n",
    "for word in vocabulary_words:\n",
    "    if word in embeddings_dict:\n",
    "        # If the words are in the embeddings, we fill them with a value\n",
    "        embedding_matrix[word_idx[word]] = embeddings_dict[word]\n",
    "if(trace):\n",
    "    print('Shape of embedding matrix:', embedding_matrix.shape)\n",
    "    print('Embedding of table', embedding_matrix[word_idx['table']])\n",
    "    print('Embedding of the padding symbol, idx 0, random numbers', embedding_matrix[0])\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(len(vocabulary_words) + 2, EMBEDDING_DIM, mask_zero=True, input_length=None))\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "# The default is True\n",
    "model.layers[0].trainable = False\n",
    "#model.add(LSTM(100, return_sequences=True))\n",
    "#model.add(Bidirectional(SimpleRNN(100, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(100, dropout = 0.25, recurrent_dropout = 0.25, return_sequences=True)))\n",
    "model.add(layers.Dropout(0.25))\n",
    "model.add(Dense(NB_CLASSES + 2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "model.summary()\n",
    "model.fit(X, Y_train, epochs=EPOCHS, batch_size=32, callbacks = [ModelCheckpoint(\"bestout.h5\", save_best_only = True)]) #sök på model checkpoint\n",
    "#lägg till validation_data tuple (x, y)\n",
    "#dev_dict = validation\n",
    "#remeber to load model!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In X_dict, we replace the words with their index\n",
    "X_test_cat, Y_test_cat = build_sequences(test_dict)\n",
    "# We create the parallel sequences of indexes\n",
    "X_test_idx = to_index(X_test_cat, word_idx)\n",
    "Y_test_idx = to_index(Y_test_cat, pos_idx)\n",
    "\n",
    "if(trace):\n",
    "    print('X[0] test idx', X_test_idx[0])\n",
    "    print('Y[0] test idx', Y_test_idx[0])\n",
    "\n",
    "X_test_padded = pad_sequences(X_test_idx)\n",
    "Y_test_padded = pad_sequences(Y_test_idx)\n",
    "if(trace):\n",
    "    print('X[0] test idx passed', X_test_padded[0])\n",
    "    print('Y[0] test idx padded', Y_test_padded[0])\n",
    "# One extra symbol for 0 (padding)\n",
    "Y_test_padded_vectorized = to_categorical(Y_test_padded, \n",
    "                                          num_classes=len(pos) + 2)\n",
    "if(trace):\n",
    "    print('Y[0] test idx padded vectorized', Y_test_padded_vectorized[0])\n",
    "    print(X_test_padded.shape)\n",
    "    print(Y_test_padded_vectorized.shape)\n",
    "\n",
    "# Evaluates with the padding symbol\n",
    "test_loss, test_acc = model.evaluate(X_test_padded, \n",
    "                                     Y_test_padded_vectorized)\n",
    "if(trace):\n",
    "    print('Loss:', test_loss)\n",
    "    print('Accuracy:', test_acc)\n",
    "\n",
    "corpus_pos_predictions = model.predict(X_test_padded)\n",
    "pos_pred_num = []\n",
    "for sent_nbr, sent_pos_predictions in enumerate(corpus_pos_predictions):\n",
    "    pos_pred_num += [sent_pos_predictions[-len(X_test_cat[sent_nbr]):]]\n",
    "print(pos_pred_num[:2])\n",
    "\n",
    "#add pos pred to test_dict\n",
    "pos_pred = []\n",
    "for sentence in pos_pred_num:\n",
    "    pos_pred_idx = list(map(np.argmax, sentence))\n",
    "    pos_pred_cat = list(map(rev_pos_idx.get, pos_pred_idx))\n",
    "    pos_pred += [pos_pred_cat]\n",
    "    i=0\n",
    "    for x in pos_pred_idx:\n",
    "       # test_dict[x].append('pred')\n",
    "        test_dict[x].append(pos_pred_cat[i])\n",
    "        i=i+1\n",
    "\n",
    "\n",
    "print(pos_pred[:2])\n",
    "print(Y_test_cat[:2])\n",
    "\n",
    "total, correct, total_ukn, correct_ukn = 0, 0, 0, 0\n",
    "for id_s, sentence in enumerate(X_test_cat):\n",
    "    for id_w, word in enumerate(sentence):\n",
    "        total += 1\n",
    "        if pos_pred[id_s][id_w] == Y_test_cat[id_s][id_w]:\n",
    "            correct += 1\n",
    "        # The word is not in the dictionary\n",
    "        if word not in word_idx:\n",
    "            total_ukn += 1\n",
    "            if pos_pred[id_s][id_w] == Y_test_cat[id_s][id_w]:\n",
    "                correct_ukn += 1\n",
    "\n",
    "print('total %d, correct %d, accuracy %f' % \n",
    "      (total, correct, correct / total))\n",
    "if total_ukn != 0:\n",
    "    print('total unknown %d, correct %d, accuracy %f' % \n",
    "          (total_ukn, correct_ukn, correct_ukn / total_ukn))\n",
    "\n",
    "sentences = [\"That round table might collapse .\", \"The man can learn well .\", \"The man can swim .\", \"The man can simwo .\"]\n",
    "for sentence in sentences:\n",
    "    y_test_pred_cat = predict_sentence(sentence.lower(), model, word_idx, vocabulary_words, rev_pos_idx, trace)\n",
    "    print(sentence)\n",
    "    print(y_test_pred_cat)\n",
    "\n",
    "save('trainout.txt', test_dict, column_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
